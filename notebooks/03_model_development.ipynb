{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development: LSTM for Order Book Forecasting\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Preparing training data from engineered features\n",
    "2. Building and training LSTM models\n",
    "3. Model evaluation and performance analysis\n",
    "4. Comparing different architectures\n",
    "\n",
    "**Objective**: Predict short-term price direction (up/down/flat) from order book microstructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.features.feature_pipeline import FeaturePipeline, FeaturePipelineConfig, create_training_dataset\n",
    "from src.models.lstm_model import OrderBookLSTM, AttentionLSTM, count_parameters\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(\"âœ… Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Data and Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic order book data\n",
    "def generate_order_book_data(n=2000):\n",
    "    np.random.seed(42)\n",
    "    snapshots = []\n",
    "    mid_price = 50000.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Add trend and noise\n",
    "        mid_price += np.random.normal(0, 10)\n",
    "        \n",
    "        bids = []\n",
    "        asks = []\n",
    "        \n",
    "        for j in range(20):\n",
    "            bid_price = mid_price - (j + 1) * 0.5\n",
    "            ask_price = mid_price + (j + 1) * 0.5\n",
    "            \n",
    "            # Volume with imbalance\n",
    "            imbalance = np.random.normal(0, 10)\n",
    "            bid_vol = max(1, 50 + imbalance + np.random.normal(0, 10))\n",
    "            ask_vol = max(1, 50 - imbalance + np.random.normal(0, 10))\n",
    "            \n",
    "            bids.append([bid_price, bid_vol])\n",
    "            asks.append([ask_price, ask_vol])\n",
    "        \n",
    "        snapshots.append({\n",
    "            'timestamp': i,\n",
    "            'exchange': 'binance',\n",
    "            'symbol': 'BTCUSDT',\n",
    "            'bids': bids,\n",
    "            'asks': asks\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(snapshots)\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating order book data...\")\n",
    "df = generate_order_book_data(n=2000)\n",
    "print(f\"âœ… Generated {len(df):,} snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "config = FeaturePipelineConfig(\n",
    "    ofi_levels=[1, 5],\n",
    "    ofi_windows=[10],\n",
    "    volatility_windows=[20],\n",
    "    ohlc_bar_size=10\n",
    ")\n",
    "\n",
    "pipeline = FeaturePipeline(config)\n",
    "\n",
    "print(\"Extracting features (this may take a moment)...\")\n",
    "features_df = pipeline.compute_all_features(df, include_volatility=False)\n",
    "print(f\"âœ… Features extracted: {len(features_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "dataset = create_training_dataset(\n",
    "    features_df,\n",
    "    prediction_horizon=50,\n",
    "    threshold_bps=5.0,\n",
    "    sequence_length=100\n",
    ")\n",
    "\n",
    "X = dataset['X']\n",
    "y = dataset['y']\n",
    "feature_names = dataset['feature_names']\n",
    "\n",
    "print(f\"\\nDataset shape:\")\n",
    "print(f\"  X: {X.shape} (samples, sequence_length, features)\")\n",
    "print(f\"  y: {y.shape} (samples,)\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    class_name = ['Down', 'Flat', 'Up'][cls]\n",
    "    print(f\"  {class_name}: {count:,} ({count/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Val: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape for scaling\n",
    "n_samples, seq_len, n_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "X_val_reshaped = X_val.reshape(-1, n_features)\n",
    "X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "\n",
    "# Fit and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped).reshape(n_samples, seq_len, n_features)\n",
    "X_val_scaled = scaler.transform(X_val_reshaped).reshape(len(X_val), seq_len, n_features)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped).reshape(len(X_test), seq_len, n_features)\n",
    "\n",
    "print(\"âœ… Features normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create PyTorch Datasets and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderBookDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = OrderBookDataset(X_train_scaled, y_train)\n",
    "val_dataset = OrderBookDataset(X_val_scaled, y_val)\n",
    "test_dataset = OrderBookDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Create loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"âœ… Data loaders created (batch_size={batch_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build and Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "input_size = n_features\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "dropout = 0.3\n",
    "\n",
    "# Create model\n",
    "model = OrderBookLSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {count_parameters(model):,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Epochs: {num_epochs}, Batch size: {batch_size}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(batch_X)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "        train_total += batch_y.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            logits, _ = model(batch_X)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "            val_total += batch_y.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print()\n",
    "\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/simulations/training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Training curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_proba = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        \n",
    "        logits, _ = model(batch_X)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        y_true.extend(batch_y.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_proba.extend(probs.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_proba = np.array(y_proba)\n",
    "\n",
    "# Classification report\n",
    "print(\"=\"*80)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_true, y_pred, target_names=['Down', 'Flat', 'Up']))\n",
    "\n",
    "# Overall accuracy\n",
    "test_accuracy = (y_true == y_pred).mean()\n",
    "print(f\"\\nOverall Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Down', 'Flat', 'Up'],\n",
    "            yticklabels=['Down', 'Flat', 'Up'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=['Down', 'Flat', 'Up'],\n",
    "            yticklabels=['Down', 'Flat', 'Up'],\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/simulations/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scaler': scaler,\n",
    "    'feature_names': feature_names,\n",
    "    'config': {\n",
    "        'input_size': input_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'num_classes': num_classes,\n",
    "        'dropout': dropout\n",
    "    },\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'history': history\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, '../models/saved/lstm_orderbook_v1.pth')\n",
    "print(\"âœ… Model checkpoint saved to: models/saved/lstm_orderbook_v1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL DEVELOPMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset:\")\n",
    "print(f\"  Total samples: {len(X):,}\")\n",
    "print(f\"  Features: {n_features}\")\n",
    "print(f\"  Sequence length: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\nðŸ¤– Model:\")\n",
    "print(f\"  Architecture: LSTM (2 layers, 128 hidden)\")\n",
    "print(f\"  Parameters: {count_parameters(model):,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Performance:\")\n",
    "print(f\"  Final train accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"  Final val accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "print(f\"  Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… Outputs:\")\n",
    "print(f\"  1. Trained model checkpoint\")\n",
    "print(f\"  2. Training curves (loss, accuracy)\")\n",
    "print(f\"  3. Confusion matrix\")\n",
    "print(f\"  4. Classification report\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"  â€¢ Implement backtesting with trained model\")\n",
    "print(f\"  â€¢ Calculate economic PnL and Sharpe ratio\")\n",
    "print(f\"  â€¢ Deploy model to API for real-time inference\")\n",
    "print(f\"  â€¢ Experiment with Transformer architecture\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
